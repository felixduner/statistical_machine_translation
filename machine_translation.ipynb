{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DAT410 - Assignment 4, NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Most frequent words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Method for removing noise in text\n",
    "def remove_noise(string):\n",
    "    \n",
    "    # Characters to be removed\n",
    "    chars = '.,!?@#%()'\n",
    "\n",
    "    # Remove special chars\n",
    "    string = string.translate(str.maketrans('','', chars))\n",
    "    \n",
    "    # Remove HTML for apostophe \n",
    "    string = re.sub(' &apos;', \"'\", string)\n",
    "\n",
    "    # Remove tags for new rows \n",
    "    string = re.sub('\\n', '', string)\n",
    "    \n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files  \n",
    "english_data = open('europarl-v7.fr-en.lc.en')\n",
    "french_data = open('europarl-v7.fr-en.lc.fr')\n",
    "\n",
    "# Read files as strings\n",
    "english_str = english_data.read()\n",
    "french_str = french_data.read()"
   ]
  },
  {
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Clean strings from special characters\n",
    "english_str = remove_noise(english_str)\n",
    "french_str = remove_noise(french_str)\n",
    "\n",
    "# Count word frequency\n",
    "en_word_counter = Counter(english_str.split())\n",
    "fr_word_coutner = Counter(french_str.split())\n",
    "\n",
    "# 10 most common words \n",
    "print(f'Most common English words: {en_word_counter.most_common(10)}')\n",
    "print(f'Most common French words: {fr_word_coutner.most_common(10)}')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most common English words: [('the', 19583), ('of', 9487), ('to', 8981), ('and', 7207), ('in', 6155), ('is', 4453), ('that', 4415), ('a', 4378), ('we', 3340), ('this', 3329)]\nMost common French words: [('de', 14520), ('la', 9736), ('et', 6617), (\"l'\", 6510), ('le', 6167), ('les', 5582), ('à', 5498), ('des', 5232), ('que', 4795), (\"d'\", 4553)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for computing probability of single word \n",
    "def word_probability(word, dict):\n",
    "    if word not in dict:\n",
    "        word_frequency = 0\n",
    "    else:\n",
    "        word_frequency = dict.get(word)\n",
    "\n",
    "    return word_frequency / sum(dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probability of the word 'speaker': 4.5943565986446645e-05\nProbability of the word 'zebra': 0.0\n"
     ]
    }
   ],
   "source": [
    "# Probability of sinlge words\n",
    "print(f\"Probability of the word 'speaker': {word_probability('speaker', en_word_counter)}\")\n",
    "print(f\"Probability of the word 'zebra': {word_probability('zebra', en_word_counter)}\")"
   ]
  },
  {
   "source": [
    "## Language modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read files as lists  \n",
    "english_sentences = open('europarl-v7.fr-en.lc.en').readlines()\n",
    "french_sentences = open('europarl-v7.fr-en.lc.fr').readlines()\n",
    "\n",
    "# Remove noise\n",
    "for index, sentence in enumerate(english_sentences):\n",
    "    english_sentences[index] = remove_noise(sentence)\n",
    "\n",
    "for index, sentence in enumerate(french_sentences):\n",
    "    french_sentences[index] = remove_noise(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating probability of initial word starting sentence\n",
    "def prob_word(word, text_lst, position):\n",
    "\n",
    "    # Changing index depending on if first/last word asked for\n",
    "    if position == 'first':\n",
    "        index = 0\n",
    "    else:\n",
    "        index = -1\n",
    "    \n",
    "    # Count nr of sentences to start with target sentence's inital word\n",
    "    count = 0 \n",
    "    \n",
    "    for i in range(len(text_lst)):\n",
    "        if text_lst[i].split()[index] == word:\n",
    "            count += 1\n",
    "\n",
    "    # If 0, set to 1 to avoid total probability equal 0\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    \n",
    "    # MLE sentences to start with target sentence's inital word\n",
    "    p_word = count/len(text_lst)\n",
    "\n",
    "    return p_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for creating bigrams of sentence\n",
    "def create_bigrams(sentence):\n",
    "\n",
    "    # Empty list to store bigrams\n",
    "    bigrams = list()\n",
    "\n",
    "    # Loop through all words in target sentence but the last\n",
    "    for i in range(0, len(sentence)-1):\n",
    "\n",
    "        # Create list for bigram\n",
    "        target_words = list()\n",
    "\n",
    "        # Append current and next word \n",
    "        target_words.append(sentence[i])\n",
    "        target_words.append(sentence[i+1])\n",
    "\n",
    "        # Join the two words separate strings into one\n",
    "        bigram = (' '.join(target_words))\n",
    "\n",
    "        bigrams.append(bigram)\n",
    "\n",
    "    return bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Method for probability of all bigrams in a sentence\n",
    "def prob_bigrams(words, bigrams, text_lst, text_str):\n",
    " \n",
    "    # Counter of all possible bigram\n",
    "    all_bigrams_count = Counter(get_bigrams(text_lst))\n",
    "\n",
    "    # Counter of all possible words\n",
    "    word_count = Counter(text_str)                  \n",
    "    \n",
    "    # Initiate probability of bigrams\n",
    "    p_bigrams = 1.0\n",
    "\n",
    "    # For each bigram ...\n",
    "    for i in range(len(bigrams)):\n",
    "        \n",
    "        # Check how many times it occurs in the complete text file\n",
    "        bigram_count = all_bigrams_count[bigrams[i]]\n",
    "\n",
    "        # If 0, set to 1 to avoid total probability equal 0\n",
    "        if bigram_count == 0:\n",
    "            bigram_count = 1\n",
    "        \n",
    "        # Check total occurencies of starting word in bigram, no matter surrounds it \n",
    "        total = word_count[words[i]]\n",
    "\n",
    "        # If 0, set to large number to avoid total probability equal 0\n",
    "        if total == 0:\n",
    "            total = 1000000\n",
    "        else:\n",
    "            total = total\n",
    "\n",
    "        # MLE for bigram \n",
    "        p_bigram = bigram_count / total \n",
    "\n",
    "        # Multiply all bigram probabilities \n",
    "        p_bigrams *= p_bigram\n",
    "\n",
    "    return p_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for getting all possible bigrams from a text file \n",
    "def get_bigrams(text_lst):\n",
    "    \n",
    "    all_bigrams = list()\n",
    "\n",
    "    # For each sentence, call create_bigrams \n",
    "    for sentence in text_lst:\n",
    "        bigram = create_bigrams(sentence.split())\n",
    "\n",
    "        # Append each bigram to all_bigrams\n",
    "        for bigram in bigram:\n",
    "            all_bigrams.append(bigram)\n",
    "\n",
    "    return all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for anguage modeling\n",
    "def language_model(sentence, text_file):\n",
    "\n",
    "    # Read files as strings (words) and lists (sentences)\n",
    "    text_str = open(text_file).read()\n",
    "    text_lst = open(text_file).readlines()\n",
    "  \n",
    "    # Clean string\n",
    "    text_str = remove_noise(text_str)\n",
    "\n",
    "    # Clean sentences \n",
    "    for index, sentence in enumerate(text_lst):\n",
    "        text_lst[index] = remove_noise(sentence)\n",
    "\n",
    "    \n",
    "    # Remove noise \n",
    "    s = remove_noise(sentence)\n",
    "\n",
    "    # Split words in target sentence\n",
    "    words = s.split()\n",
    "\n",
    "    # Get probability of inital word starting a sentence\n",
    "    p_first = prob_word(words[0], text_lst, 'first')\n",
    "\n",
    "    # Get probability of final word ending a sentence\n",
    "    p_last = prob_word(words[-1], text_lst, 'last')\n",
    "    \n",
    "    # Get bigrams of words in sentence\n",
    "    bigrams = create_bigrams(words)\n",
    "\n",
    "    # Get probability of all bigrams in target sentence\n",
    "    p_bigrams = prob_bigrams(words, bigrams, text_lst, text_str)\n",
    "\n",
    "    # Multiply probabilities to get final results\n",
    "    p_sentence = p_first * p_bigrams * p_last\n",
    "\n",
    "    return p_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test sentence: i declare resumed the session of the european parliament adjourned on friday 17 december 1999  and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period \n",
      "Probability of a sentence starting with the word 'I':0.1141\n",
      "Probability of a sentence ending with the word 'last': 0.001\n",
      "Probability of bigrams: 1.3596761261666992e-54\n",
      "Test sentence: i declare resumed the session of the european parliament adjourned on friday 17 december 1999  and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period \n",
      "Probability of sentence: 8.72028988363961e-81\n"
     ]
    }
   ],
   "source": [
    "# Test sentence from text file \n",
    "s = english_sentences[0]\n",
    "print(f\"Test sentence: {s}\")\n",
    "\n",
    "# UNIT TEST: prob_word\n",
    "print(f\"Probability of a sentence starting with the word 'I':{prob_word('i', english_sentences, 'first')}\")\n",
    "\n",
    "print(f\"Probability of a sentence ending with the word 'last': {prob_word('period', english_sentences, 'last')}\")\n",
    "\n",
    "# UNIT TEST: create_bigrams\n",
    "s_bigrams = create_bigrams(s.split())\n",
    "\n",
    "# UNIT TEST: prob_bigrams\n",
    "print(f\"Probability of bigrams: {prob_bigrams(s.split(), s_bigrams, english_sentences, english_str.split())}\")\n",
    "\n",
    "# UNIT TEST: get_bigram\n",
    "get_bigrams(english_sentences)\n",
    "\n",
    "# UNIT TEST: language_model\n",
    "print(f\"Test sentence: {s}\")\n",
    "print(f\"Probability of sentence: {language_model(sentence=s, text_file='europarl-v7.fr-en.lc.en')}\")"
   ]
  },
  {
   "source": [
    "## Translation model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating word alignment probabilities for a sentence pair \n",
    "def alignment_prob(t_prob):\n",
    "\n",
    "    # Empty dict to store sum of word alignment probabilities\n",
    "    source_sums = dict()\n",
    "\n",
    "    delta = t_prob.copy()\n",
    "\n",
    "    # Calculating sum of word alignment probabilities (divider) for each source word\n",
    "    for (source, target), prob in delta.items():\n",
    "        if source in source_sums.keys():\n",
    "            source_sums[source] += prob\n",
    "        else:\n",
    "            source_sums[source] = prob \n",
    "            \n",
    "    # Calculating word alignment probabilities for each source word\n",
    "    for (source, target), prob in delta.items():\n",
    "\n",
    "        # Sum of word alignment probabilities \n",
    "        total = source_sums[source]\n",
    "\n",
    "        # New word alignment probabilities\n",
    "        delta[(source, target)] = prob / total \n",
    "    \n",
    "    return delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize transition probabilities and soft counts for unit tests\n",
    "import itertools\n",
    "from random import random\n",
    "\n",
    "# Read files as strings (words) and lists (sentences)\n",
    "source_str = open('fr_test.txt').read()    \n",
    "target_str = open('en_test.txt').read()\n",
    "\n",
    "# Clean strings\n",
    "source_str = remove_noise(source_str)\n",
    "target_str = remove_noise(target_str)\n",
    "\n",
    "# Split corpus into words\n",
    "source_words = source_str.split()\n",
    "target_words = target_str.split()\n",
    "\n",
    "# Add NULL to English words\n",
    "target_words.append('NULL')\n",
    "\n",
    "# Initialize t(f|e), c(f, e), c(e) for complete corpus \n",
    "test_trans_prob = dict()\n",
    "test_soft_counts = dict()\n",
    "\n",
    "iterables = [source_words, target_words]\n",
    "for t in itertools.product(*iterables):\n",
    "\n",
    "    # Randomize inital transition probabilities \n",
    "    test_trans_prob[t] = random()    \n",
    "\n",
    "    # Initialize all soft_counts to 0 \n",
    "    test_soft_counts[t] = 0\n",
    "    test_soft_counts[t[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6575734810977056\n0.0031864720801970284\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST: alignment_prob\n",
    "print(test_trans_prob[('décembre', '1999')])\n",
    "test_delta = alignment_prob(test_trans_prob)\n",
    "print(test_delta[('décembre', '1999')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for updating soft counts\n",
    "def update_counts (soft_counts, delta):\n",
    "\n",
    "    for (source, target), align_prob in delta.items():\n",
    "        soft_counts[(source, target)] += align_prob\n",
    "        soft_counts[target] += align_prob\n",
    "\n",
    "    return soft_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n0.002309010501550066\n1.119198031707437\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST: update_counts\n",
    "\n",
    "print(test_soft_counts[('vous', 'to')])\n",
    "print(test_soft_counts['to'])\n",
    "\n",
    "x = update_counts(test_soft_counts, test_delta)\n",
    "\n",
    "print(x[('vous', 'to')])\n",
    "print(x['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for updating transition probabilities \n",
    "def transition_prob (t_prob, soft_counts):\n",
    "    \n",
    "    for (source, target), prob in t_prob.items():\n",
    "        t_prob[(source, target)] = soft_counts[(source, target)] / soft_counts[target]\n",
    "\n",
    "    return t_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6575734810977056\n",
      "0.0029588486383488968\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST: transition_prob\n",
    "\n",
    "print(test_trans_prob[('décembre', '1999')])\n",
    "\n",
    "res = transition_prob(test_trans_prob, test_soft_counts)\n",
    "\n",
    "print(res[('décembre', '1999')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Translation model \n",
    "def translation_model(source_file, target_file, n_iter):\n",
    "\n",
    "    # Read files as strings (words) and lists (sentences)\n",
    "    source_str = open(source_file).read()\n",
    "    target_str = open(target_file).read()\n",
    "\n",
    "    source_list = open(source_file).readlines()\n",
    "    target_list = open(target_file).readlines()\n",
    "\n",
    "    # Clean strings\n",
    "    source_str = remove_noise(source_str)\n",
    "    target_str = remove_noise(target_str)\n",
    "\n",
    "    # Clean sentences \n",
    "    for index, sentence in enumerate(target_list):\n",
    "        target_list[index] = remove_noise(sentence)\n",
    "\n",
    "    for index, sentence in enumerate(source_list):\n",
    "        source_list[index] = remove_noise(sentence)\n",
    "\n",
    "    # Split corpus into words\n",
    "    source_words = source_str.split()\n",
    "    target_words = target_str.split()\n",
    "\n",
    "     # Add NULL to English words\n",
    "    target_words.append('NULL')\n",
    "\n",
    "    # Initialize t(f|e), c(f, e), c(e) for complete corpus \n",
    "    trans_prob = dict()\n",
    "    soft_counts = dict()\n",
    "\n",
    "    # For each iteration\n",
    "    for i in range(n_iter):\n",
    "    # For each sentence pair\n",
    "        for (source, target) in zip(source_list, target_list):\n",
    "    \n",
    "            # Split sentences into words\n",
    "            source_sentence = source.split()\n",
    "            target_sentence = target.split()\n",
    "\n",
    "            # Add NULL to English sentence \n",
    "            target_sentence.append('NULL')\n",
    "\n",
    "            # Initialize t(f|e), c(f, e), c(e) for specific sentences \n",
    "            # To perform operations on small dictionaries one at a time\n",
    "            temp_trans_prob = dict()\n",
    "            temp_soft_counts = dict()\n",
    "\n",
    "            iterables = [source_sentence, target_sentence]\n",
    "            for t in itertools.product(*iterables):\n",
    "\n",
    "                # Get transition probabilities from general dictionary if present, otherwise initialize to random [0,1)\n",
    "                if t in trans_prob.keys():\n",
    "                    temp_trans_prob[t] =  trans_prob[t]\n",
    "                else:\n",
    "                    temp_trans_prob[t] = random()\n",
    "                \n",
    "                # Get soft counts from general dictionary, otherwise initialize to 0 \n",
    "                if t in soft_counts.keys():\n",
    "                    temp_soft_counts[t] = soft_counts[t]\n",
    "                else:\n",
    "                    temp_soft_counts[t] = 0\n",
    "\n",
    "                if t[1] in soft_counts.keys():\n",
    "                    temp_soft_counts[t[1]] = soft_counts[t[1]]\n",
    "                else: \n",
    "                    temp_soft_counts[t[1]] = 0\n",
    "\n",
    "\n",
    "            # Get word alignment probabilities\n",
    "            delta = alignment_prob(temp_trans_prob) \n",
    "\n",
    "            # Update soft counts \n",
    "            temp_soft_counts = update_counts(temp_soft_counts, delta)\n",
    "\n",
    "            # Get transitional probabilities \n",
    "            temp_trans_prob = transition_prob(temp_trans_prob, temp_soft_counts)\n",
    "\n",
    "            # Update general dictionaries\n",
    "            for key, value in temp_trans_prob.items():\n",
    "                trans_prob[key] = value\n",
    "            \n",
    "            \n",
    "            for key, value in temp_soft_counts.items():\n",
    "                soft_counts[key] = value\n",
    "\n",
    "    # Sort transition probabilities by descending order\n",
    "    trans_prob = dict(sorted(trans_prob.items(), key=operator.itemgetter(1),reverse=True))\n",
    "  \n",
    "    return trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model: Get transition probabilities dictionary \n",
    "trans_prob_1 = translation_model(source_file='europarl-v7.fr-en.lc.fr', target_file='europarl-v7.fr-en.lc.en', n_iter=1)\n",
    "trans_prob_5 = translation_model(source_file='europarl-v7.fr-en.lc.fr', target_file='europarl-v7.fr-en.lc.en', n_iter=5)\n",
    "trans_prob_10 = translation_model(source_file='europarl-v7.fr-en.lc.fr', target_file='europarl-v7.fr-en.lc.en', n_iter=10)\n",
    "trans_prob_20 = translation_model(source_file='europarl-v7.fr-en.lc.fr', target_file='europarl-v7.fr-en.lc.en', n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for getting the n most likely translations for a specific word \n",
    "def word_translation (word, trans_prob, n):\n",
    "\n",
    "    translations = list()\n",
    "\n",
    "    for (source, target), prob in trans_prob.items():               \n",
    "        if source == word:\n",
    "            translations.append(target)       \n",
    "    \n",
    "    return translations[:n]"
   ]
  },
  {
   "source": [
    "### Most probable translation of the French word 'européenne' for different EM iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Most likely English translations the french word 'européenne' after 1 iteration:\n",
      " ['multi-governmental', 'enables', 'blockade', 'communitise', 'impossible', 'enrich', 'reinvent', 'acceding', \"soul'\", 'county']\n",
      "\n",
      "Most likely English translations the french word 'européenne' after 5 iterations:\n",
      " ['european', 'aided', 'symposium', 'contend', 'turns', 'europe-wide', 'overburdened', 'chapeau', 'provoke', 'adds']\n",
      "\n",
      "Most likely English translations the french word 'européenne' after 10 iterations:\n",
      " ['european', 'aided', 'turns', 'patchwork', 'embellish', 'adds', 'armaments', 'periodical', 'qualms', 'focuses']\n",
      "\n",
      "Most likely English translations the french word 'européenne' after 20 iterations:\n",
      " ['european', 'turns', 'aided', 'symposium', 'embellish', 'conceded', \"europe'\", 'de-europeanise', 'europe-wide', 'agendas']\n"
     ]
    }
   ],
   "source": [
    "# Using transition probabilitites generated after 1 iteration\n",
    "print(f\"\\nMost likely English translations the french word 'européenne' after 1 iteration:\\n {word_translation('européenne', trans_prob_1, 10)}\")\n",
    "\n",
    "# Using transition probabilitites generated after 5 iteration\n",
    "print(f\"\\nMost likely English translations the french word 'européenne' after 5 iterations:\\n {word_translation('européenne', trans_prob_5, 10)}\")\n",
    "\n",
    "# Using transition probabilitites generated after 10 iteration\n",
    "print(f\"\\nMost likely English translations the french word 'européenne' after 10 iterations:\\n {word_translation('européenne', trans_prob_10, 10)}\")\n",
    "\n",
    "# Using transition probabilitites generated after 20 iteration\n",
    "print(f\"\\nMost likely English translations the french word 'européenne' after 20 iterations:\\n {word_translation('européenne', trans_prob_20, 10)}\")"
   ]
  },
  {
   "source": [
    "## Decoding "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Method for translating sentence\n",
    "def translate (sentence, target_text):\n",
    "\n",
    "    # Store all words in list\n",
    "    words = list()\n",
    "\n",
    "    for source_word in sentence.split():\n",
    "\n",
    "        # Get most likely translated word using the best trans_prob dictionary \n",
    "        target_word = word_translation(source_word, trans_prob_20, 1)\n",
    "\n",
    "        words.extend(target_word)\n",
    "\n",
    "    # Create list with all possible sentences with translated words\n",
    "    #print(words)\n",
    "    permutations = list(itertools.permutations(words))\n",
    "    \n",
    "\n",
    "    # Calculate probability of each possible sentence\n",
    "    results = dict()\n",
    "\n",
    "    # Get probability of each permutation\n",
    "    for sentence in permutations:\n",
    "        sentence = ' '.join(sentence)\n",
    "        res = language_model(sentence=sentence, text_file=target_text)\n",
    "        results[sentence] = res\n",
    "\n",
    "    # Get most probable sentence \n",
    "    translation = max(results, key=results.get)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "French: je suis très heureux \n English translation: i am very pleased\n"
     ]
    }
   ],
   "source": [
    "# Test sentence in French to be translated into English\n",
    "print(f\"French: je suis très heureux \\n English translation: {translate('je suis très heureux', 'europarl-v7.fr-en.lc.en')}\")"
   ]
  }
 ]
}